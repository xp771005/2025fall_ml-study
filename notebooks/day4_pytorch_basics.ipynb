{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f44953-00c1-4319-bb00-5a685950a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d8c9cbd-f151-4296-ba3a-b78620a0f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) torch.Size([3]) torch.float32\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.5412, 0.9746],\n",
      "        [0.4062, 0.1236]])\n",
      "tensor([[-0.1903, -0.7232],\n",
      "        [-0.1780,  0.5354]])\n"
     ]
    }
   ],
   "source": [
    "# tensor 就是 PyTorch 的“矩阵/向量容器”\n",
    "a = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "b = torch.zeros((2, 3))    # 2x3 矩阵，全 0\n",
    "c = torch.ones((2, 2))     # 全 1\n",
    "d = torch.rand((2, 2))     # 0~1 均匀分布\n",
    "e = torch.randn((2, 2))    # 正态分布\n",
    "\n",
    "# 看看形状和类型\n",
    "print(a, a.shape, a.dtype)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b49003-28f7-47f2-97a1-de4e3aab415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) torch.Size([3]) torch.float32\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) torch.Size([2, 3])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.6043, 0.3888],\n",
      "        [0.4953, 0.4694]])\n",
      "tensor([[ 1.4588,  1.3292],\n",
      "        [-1.6625, -0.1007]])\n"
     ]
    }
   ],
   "source": [
    "print(a, a.shape, a.dtype)\n",
    "print(b, b.shape)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26e45f-da7d-47e9-a9b1-4fde48a720dc",
   "metadata": {},
   "source": [
    "\n",
    "tensor vs NumPy array\n",
    "  - tensor 是 PyTorch 的基本数据结构，可以直接在 GPU 上运行；  \n",
    "  - NumPy array 只能在 CPU 上运行，不能自动求导。  \n",
    "rand vs randn\n",
    "  - `rand` 生成 [0,1) 区间的均匀分布随机数；  \n",
    "  - `randn` 生成均值=0、方差=1 的正态分布随机数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9214ee8-5149-4735-a0dd-f212dfd5e371",
   "metadata": {},
   "source": [
    "用 PyTorch 的 autograd 机制，计算一个简单的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ac54fd-138b-4d8b-bdc3-6177b40f71e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度 a.grad: tensor([4., 5.])\n"
     ]
    }
   ],
   "source": [
    "# --- Cell (Code) ---\n",
    "a = torch.tensor([2.0, 3.0], requires_grad=True)  # 参数 a\n",
    "b = torch.tensor([4.0, 5.0])                      # 常量 b\n",
    "c = (a * b).sum()                                 # 损失函数 c = 2*4 + 3*5 = 23\n",
    "c.backward()                                      # 自动计算梯度\n",
    "\n",
    "print(\"梯度 a.grad:\", a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d57c9-9c74-4d52-8714-c7b277feae3e",
   "metadata": {},
   "source": [
    "在神经网络里，这样的自动求导有什么用？\n",
    "  - 神经网络的损失函数 L 对所有参数 θ 自动求导 → 得到梯度 → 用梯度下降更新参数。  \n",
    "  - 避免手写复杂的链式法则，大大简化训练过程。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94d4f2-6706-43ad-a35c-5903d99edde2",
   "metadata": {},
   "source": [
    "写一个最小模型，用梯度下降拟合 y=2x+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d40dbab-bef9-4900-a253-8dcf0f929a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: loss=1.3011, w=2.0000, b=1.8821\n",
      "Epoch 100: loss=0.1726, w=2.0000, b=2.5929\n",
      "Epoch 150: loss=0.0229, w=2.0000, b=2.8518\n",
      "Epoch 200: loss=0.0030, w=2.0000, b=2.9460\n"
     ]
    }
   ],
   "source": [
    "# 数据：y = 2x + 3\n",
    "x = torch.linspace(-5, 5, 20).unsqueeze(1)  # shape: (20, 1) 20 行、每行 1 个特征”的小批量数据\n",
    "y = 2 * x + 3\n",
    "\n",
    "# 参数初始化随机，让他拟合实际值\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(200):\n",
    "    # 前向传播\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y) ** 2).mean()  # MSE 均方误差\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 参数更新 (手动 SGD)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # 梯度清零\n",
    "    # PyTorch 的梯度是累积的（accumulate），如果不清零，下一轮会把新旧梯度加在一起\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss={loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616c524-1b59-468a-bf11-6f84c3c0f25d",
   "metadata": {},
   "source": [
    "为什么要按照 Forward → Loss → Backward → Update 的顺序来做：\n",
    "\n",
    "Forward（前向传播）：输入数据经过模型计算输出（预测值）。\n",
    "\n",
    "Loss（损失计算）：比较预测值和真实值，算出误差。\n",
    "\n",
    "Backward（反向传播）：通过链式法则计算损失对参数的梯度。\n",
    "\n",
    "Update（参数更新）：利用梯度下降法调整参数，让下一次预测更接近真实值。\n",
    "\n",
    "整个过程像一个闭环，重复很多次，模型就越来越“聪明”。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
